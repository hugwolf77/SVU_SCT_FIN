{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38af8690-821a-4296-a00b-fa6eefab76f7",
   "metadata": {
    "id": "38af8690-821a-4296-a00b-fa6eefab76f7"
   },
   "source": [
    "<span style=\"color:#ffd33d; font-size:150%\"> 2023.07.20: DLinear Model Demo </span>\n",
    "## 2023.07.13\n",
    "BASE) _Kick_Off_\n",
    "## 2023.07.17\n",
    "BASE)_BIVA\n",
    " - model edit\n",
    " - BIVA-RITS, BIVA-BRITS, BIVA-VAE, BIVA,\n",
    "## 2023.07.19\n",
    " - main, datalorder\n",
    "## 2023.07.20\n",
    " - ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "MmKNyZB4kwVC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2622,
     "status": "ok",
     "timestamp": 1689258818014,
     "user": {
      "displayName": "김의철",
      "userId": "14673159290581005242"
     },
     "user_tz": -540
    },
    "id": "MmKNyZB4kwVC",
    "outputId": "9e3f1e3a-aa03-4de7-b1ad-55e6b3766508"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/ZZ/Code_02/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9497d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hugwolf77/SVU_SCT_FIN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61a8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/SVU_SCT_FIN/Model/BIVA/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207dac3",
   "metadata": {},
   "source": [
    "SVU_SCT_FIN/Model/BIVA/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ba215-d54c-4d23-a058-cb40e75ffcc2",
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1689258822079,
     "user": {
      "displayName": "김의철",
      "userId": "14673159290581005242"
     },
     "user_tz": -540
    },
    "id": "b65ba215-d54c-4d23-a058-cb40e75ffcc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 13:42:44.601008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-03 13:42:44.731735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-03 13:42:44.797389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-03 13:42:44.819743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-03 13:42:44.921211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 13:42:46.653159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))\n",
    "import io\n",
    "from pickle import FALSE\n",
    "\n",
    "from datetime import date, datetime\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from typing import Set, List, Dict, Tuple #, final\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pickle import FALSE\n",
    "\n",
    "from model.exp_main import Exp_Main\n",
    "# from model.exp_stat import Exp_Main as exp_stats\n",
    "from model.utils.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# from sklearn.metrics import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker  \n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "import IPython.display\n",
    "mpl.rcParams['figure.figsize'] = (16,12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "q-sQpnWRCj2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1689258825551,
     "user": {
      "displayName": "김의철",
      "userId": "14673159290581005242"
     },
     "user_tz": -540
    },
    "id": "q-sQpnWRCj2S",
    "outputId": "130b78ac-509a-4cb3-fea5-090ef0b6e139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/augustine77/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.path.dirname(os.getcwd())\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29b553d-f3db-45c9-b52a-25429196cbd4",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "b29b553d-f3db-45c9-b52a-25429196cbd4",
    "outputId": "79ae6456-b6eb-4661-f162-980907db927d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "<class '__main__.args'>\n",
      "\n",
      " train start -----------------------> 1\n",
      "\n",
      "Use CPU\n",
      ">>>>>>>start training : BIVA-no1-dt20250603-seq6-pred1-bt5-lr0.5-mv6_BIVA_seq6_pred1_bt5_lr0.5_102>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 220\n",
      "val 30\n",
      "test 29\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 127\u001b[39m\n\u001b[32m    125\u001b[39m exp = Exp_Main(args)  \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[33m'\u001b[39m.format(setting))\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[33m'\u001b[39m.format(setting))\n\u001b[32m    130\u001b[39m preds, trues, inputx, mae, mse, rmse, mape, mspe, rse, corr = exp.test(setting)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/model/exp_main.py:130\u001b[39m, in \u001b[36mExp_Main.train\u001b[39m\u001b[34m(self, setting)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# batch_x_mark = batch_x_mark.float().to(self.device)\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# batch_y_mark = batch_y_mark.float().to(self.device)\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mBIVA\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.model:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     states, VAE_loss, recon_output, seasonal_init, imputed_loss, imputed_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/model/BIVA.py:205\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    203\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.RIN_func.set_RIN(x)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# BRITS - imputation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mBRITS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m imputed_x = x[\u001b[33m'\u001b[39m\u001b[33mimputations\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    207\u001b[39m imputed_loss = x[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/model/BIVA_BRITS.py:23\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     ret_f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrits_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mforward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     ret_b = \u001b[38;5;28mself\u001b[39m.reverse(\u001b[38;5;28mself\u001b[39m.rits_b(data, \u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     25\u001b[39m     imputations = \u001b[38;5;28mself\u001b[39m.merge_ret(ret_f, ret_b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/sim-0TBU-pA2-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/model/BIVA_RITS.py:164\u001b[39m, in \u001b[36mModel.forward\u001b[39m\u001b[34m(self, data, direct)\u001b[39m\n\u001b[32m    161\u001b[39m gamma_h = \u001b[38;5;28mself\u001b[39m.temp_decay_h(d)\n\u001b[32m    162\u001b[39m gamma_x = \u001b[38;5;28mself\u001b[39m.temp_decay_x(d)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m h = \u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_h\u001b[49m\n\u001b[32m    165\u001b[39m x_h = \u001b[38;5;28mself\u001b[39m.hist_reg(h)\n\u001b[32m    166\u001b[39m x_h = F.sigmoid(x_h)\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "fix_seed = 2025\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "# argparser 에서 class 형태로 변경.\n",
    "class args:\n",
    "    pass\n",
    "\n",
    "save_path = '/home/augustine77/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/exp'\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "result_df = pd.DataFrame(columns=['no','model_id','root','len_seq','len_pred',\n",
    "                                  'learn_rate','batch','MSE','MAE','RMSE','MAPE'])\n",
    "no = 0\n",
    "\n",
    "len_seq_list = [6] #,9,12,15,18,21,24]\n",
    "len_pred_list = [1] #,2,3,4]\n",
    "batch_list = [5]\n",
    "lr_rate_list = [0.5]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for len_seq in len_seq_list:\n",
    "  for len_pred in len_pred_list:\n",
    "    for batch in batch_list:\n",
    "      for lr_rate in lr_rate_list:\n",
    "        no += 1\n",
    "\n",
    "        now = time.localtime()\n",
    "        time_now = time.strftime('%Y%m%d', now)\n",
    "\n",
    "        args.is_training = True    # 'status'\n",
    "        args.model = 'BIVA'  # 'model name'\n",
    "        args.model_id = '{}-no{}-dt{}-seq{}-pred{}-bt{}-lr{}-mv6'.format(args.model,no,time_now,len_seq,len_pred,batch,lr_rate)\n",
    "\n",
    "        # dataset\n",
    "        args.data = 'BIVA'                                                                                             # 'dataset type : data_loader'\n",
    "        args.root_path = '/home/augustine77/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/Base_data'\n",
    "        args.data_path = 'dataset_03_S.xlsx'\n",
    "\n",
    "        args.des = 'Exp'                                                                                                 # 'exp description'\n",
    "        args.loss = 'mse'                                                                                                # loss function\n",
    "        args.lradj = '1' #'dtype1'                                                                                             # adjusting learning rate option\n",
    "        args.use_amp = False                                                                                             # 'use automatic mixed precision training'\n",
    "\n",
    "        args.features = 'MS'                                                                                             # 'forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate'\n",
    "        args.target = 'A1' # 'GDP'                                                                                       # 'target feature in S or MS task')\n",
    "        args.freq = 'M'                                                                                                  # 'freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h'\n",
    "        args.checkpoints =  '/home/augustine77/mylab/sim/sim/05_SVU_SCT_FIN/Model/BIVA/exp/(BIVA)_checkpoints/'                                   #'location of model checkpoints'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = len_seq                                                                                           # 'input sequence length'\n",
    "        args.pred_len = len_pred\n",
    "        args.label_len = 1\n",
    "\n",
    "        args.channels = 102\n",
    "        args.rnn_hid_size = 256\n",
    "        args.vae_hid_size = 256    ## deprivation\n",
    "        args.vae_latent_size = 14\n",
    "        args.infer_hid_size = 128\n",
    "        args.moving_avg = 6                                                                                    # 'window size of moving average'\n",
    "        # args.moving_avg = [6,10,15,20,25,30,35,40,45,60]                                                              # 'window size' list of moving average'\n",
    "\n",
    "\n",
    "        args.cols = None\n",
    "\n",
    "        #\n",
    "        args.conv1d = True\n",
    "        args.conv_kernal = 1\n",
    "        args.RIN = False\n",
    "        args.combination = True\n",
    "\n",
    "        args.imputed_weight = 0.5\n",
    "\n",
    "        args.last_chkpt = None #'BIVA-no1-dt20230801-seq6-pred1-bt5-lr0.5-mv6_last_checkpoint.pth'\n",
    "        args.learning_rate = lr_rate\n",
    "        args.batch_size = batch\n",
    "        args.train_epochs = 2           # 'train epochs'\n",
    "        args.patience = 10              # 'early stopping patience'\n",
    "\n",
    "        args.embed = 'timeF'            # 'time features encoding, options:[timeF, fixed, learned]'\n",
    "        args.inverse = False\n",
    "        # args.activation = 'relu'        # 'activation'\n",
    "        args.do_predict = False         # 'whether to predict unseen future data')\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10            # 'data loader num workers'\n",
    "        args.itr = 1                    # 'experiments times'\n",
    "\n",
    "        # GPU\n",
    "        args.use_gpu = False             # 'use gpu'\n",
    "        args.gpu = 0                    # 'gpu'\n",
    "        args.use_multi_gpu = False      # 'use multiple gpus'\n",
    "        args.devices = '0,1,2,3'        # 'device ids of multile gpus'\n",
    "        # args.test_flop = False          # 'See utils/tools for usage'\n",
    "\n",
    "        args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "        if args.use_gpu and args.use_multi_gpu:\n",
    "            args.dvices = args.devices.replace(' ', '')\n",
    "            device_ids = args.devices.split(',')\n",
    "            args.device_ids = [int(id_) for id_ in device_ids]\n",
    "            args.gpu = args.device_ids[0]\n",
    "\n",
    "        print('Args in experiment:')\n",
    "        print(args)\n",
    "        #-----------------------------------------------------------------------------------------------------------------------\n",
    "        print(f\"\\n train start -----------------------> {no}\\n\")\n",
    "        #-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        if args.is_training:\n",
    "            start_time = time.time()\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_seq{}_pred{}_bt{}_lr{}_{}'.format(\n",
    "                    args.model_id,\n",
    "                    args.data,\n",
    "                    args.seq_len,\n",
    "                    args.pred_len,\n",
    "                    args.batch_size,\n",
    "                    args.learning_rate,\n",
    "                    args.channels,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp_Main(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                preds, trues, inputx, mae, mse, rmse, mape, mspe, rse, corr = exp.test(setting)\n",
    "\n",
    "                if args.do_predict:\n",
    "                    print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                    exp.predict(setting, True)\n",
    "\n",
    "                time_cost = str(time.time() - start_time)\n",
    "\n",
    "                result = {'no':no+1,'Model_id':args.model_id,'root':args.root_path,'len_seq':args.seq_len,'len_pred':args.pred_len,\n",
    "                          'learn_rate':args.learning_rate,'batch':args.batch_size,\n",
    "                          'MSE':mse,'MAE':mae,'RMSE':rmse, 'MAPE':mape,'Corr':corr, 'time_cost': time_cost}\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            ii = 0\n",
    "            setting = '{}_{}_seq{}_pred{}_bt{}_lr{}_{}'.format(\n",
    "                    args.model_id,\n",
    "                    args.data,\n",
    "                    args.seq_len,\n",
    "                    args.pred_len,\n",
    "                    args.batch_size,\n",
    "                    args.learning_rate,\n",
    "                    args.channels,\n",
    "                    args.des, ii)\n",
    "\n",
    "            exp = Exp_Main(args)  # set experiments\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            preds, trues, inputx, mae, mse, rmse, mape, mspe, rse, corr = exp.test(setting, test=1)\n",
    "\n",
    "            result = {'no':no+1,'Model_id':args.model_id,'root':args.root_path,'len_seq':args.seq_len,'len_pred':args.pred_len,\n",
    "                          'learn_rate':args.learning_rate,'batch':args.batch_size,\n",
    "                          'MSE':mse,'MAE':mae,'RMSE':rmse, 'MAPE':mape,'Corr':corr, 'time_cost': time_cost}\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        result_df = pd.concat([result_df,pd.DataFrame(result)], ignore_index = True)\n",
    "        result_df.to_csv(os.path.join(save_path,'(BIVA)_{}_CV_no{}_exp01.csv'.format(args.model_id,no)))\n",
    "        print(f\"\\n train done -----------------------> {no} ---- {time_cost} \\n\")\n",
    "        no += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "sim-0TBU-pA2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
